{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/xli27/GitHub/gym\")\n",
    "import gym\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n",
      "3\n",
      "[20, 20]\n",
      "[0.09  0.007]\n",
      "(20, 20, 3)\n",
      "[19 15]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env = env.unwrapped\n",
    "\n",
    "env.reset()\n",
    "\n",
    "print(env.observation_space.high) # position and velocity # [0.6  0.07]\n",
    "print(env.observation_space.low) # [-1.2  -0.07]\n",
    "print(env.action_space.n) # 3\n",
    "\n",
    "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
    "DISCRETE_OS_WIN_SIZE = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
    "\n",
    "print(DISCRETE_OS_SIZE) # [20, 20]\n",
    "print(DISCRETE_OS_WIN_SIZE) # [0.09  0.007]\n",
    "\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE) + [env.action_space.n])\n",
    "#\n",
    "# \n",
    "print(q_table.shape)\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low) / DISCRETE_OS_WIN_SIZE\n",
    "    return tuple(discrete_state.astype(np.int))\n",
    "\n",
    "num_states = (env.observation_space.high - env.observation_space.low)*np.array([10, 100])\n",
    "num_states = np.round(num_states, 0).astype(int) + 1\n",
    "\n",
    "print(num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50 100]\n"
     ]
    }
   ],
   "source": [
    "ar1 = [10, 10]\n",
    "ar2 = ar1 * np.array([5, 10])\n",
    "print (ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.56237947  0.        ]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "state_adj = np.round(state_adj).astype(int)\n",
    "print(state_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.64828499 -1.3857378  -1.63248489]\n",
      "  [-1.36816649 -0.68144631 -1.75725941]\n",
      "  [-0.72199281 -1.01950546 -0.2460835 ]\n",
      "  [-1.23131631 -1.68171993 -1.19367264]\n",
      "  [-0.34164498 -0.93032606 -1.33756734]\n",
      "  [-1.10633938 -1.74928131 -1.0564066 ]]\n",
      "\n",
      " [[-0.86715997 -0.35308019 -0.9662984 ]\n",
      "  [-1.84012626 -0.16143101 -0.17780571]\n",
      "  [-1.24973509 -0.49032829 -1.24213753]\n",
      "  [-0.06545747 -1.77101149 -0.47102297]\n",
      "  [-1.86455716 -0.22547746 -0.56819628]\n",
      "  [-0.97444298 -1.82396288 -0.36710203]]\n",
      "\n",
      " [[-0.30077782 -1.03911875 -1.62533328]\n",
      "  [-1.09779731 -1.90086246 -1.45308893]\n",
      "  [-1.48841338 -0.5229928  -0.49726891]\n",
      "  [-1.48883444 -1.7832326  -1.73195826]\n",
      "  [-1.35542131 -1.55118296 -1.43785116]\n",
      "  [-0.11328219 -1.33505522 -1.74822582]]\n",
      "\n",
      " [[-0.63415048 -0.0585072  -0.65537454]\n",
      "  [-0.72043237 -0.23648866 -1.8312573 ]\n",
      "  [-0.90054045 -1.30012142 -0.26058758]\n",
      "  [-1.24293802 -0.87643941 -0.68681632]\n",
      "  [-1.66703929 -1.39081955 -0.51222942]\n",
      "  [-0.46447727 -1.53645664 -1.71459455]]\n",
      "\n",
      " [[-1.46306381 -0.72228017 -1.34454953]\n",
      "  [-1.25418223 -0.52956676 -0.01414298]\n",
      "  [-1.42654873 -1.36693612 -0.78945321]\n",
      "  [-1.42942871 -0.47557139 -0.27829348]\n",
      "  [-1.73512792 -0.15330188 -1.0577893 ]\n",
      "  [-1.98199179 -0.85511998 -1.12957236]]]\n"
     ]
    }
   ],
   "source": [
    "q_table = np.random.uniform(low=-2, high=0, size=([5,6]) + [env.action_space.n])\n",
    "#\n",
    "# \n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "\n",
    "action = np.argmax(q_table[2,0]) \n",
    "print (action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.48841338 -0.5229928  -0.49726891]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.40078756,  0.        ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q_table[2,2]) \n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.41303211, -0.00436988]), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "print(env.step(np.random.randint(0, env.action_space.n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1000, reward -1.0\n",
      "it 2000, reward -1.0\n",
      "it 3000, reward -1.0\n",
      "it 4000, reward -1.0\n",
      "it 5000, reward -1.0\n",
      "it 6000, reward -1.0\n",
      "it 7000, reward -1.0\n",
      "it 8000, reward -1.0\n",
      "it 9000, reward -1.0\n",
      "it 10000, reward -1.0\n",
      "it 11000, reward -1.0\n",
      "it 12000, reward -1.0\n",
      "it 13000, reward -1.0\n",
      "it 14000, reward -1.0\n",
      "it 15000, reward -1.0\n",
      "it 16000, reward -1.0\n",
      "it 17000, reward -1.0\n",
      "it 18000, reward -1.0\n",
      "it 19000, reward -1.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "num_states = (env.observation_space.high - env.observation_space.low)*np.array([10, 100])\n",
    "num_states = np.round(num_states, 0).astype(int) + 1 # [19 15]\n",
    "\n",
    "# Initialize Q table\n",
    "Q = np.random.uniform(low = -1, high = 1, \n",
    "                      size = (num_states[0], num_states[1], \n",
    "                              env.action_space.n))\n",
    "    \n",
    "# Discretize state\n",
    "state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "state_adj = np.round(state_adj, 0).astype(int) ## numpy.around(a, decimals=0, out=None ## like [6 7]\n",
    "\n",
    "done = False\n",
    "\n",
    "epsilon = 0.8\n",
    "learning = 0.2\n",
    "discount = 0.9\n",
    "\n",
    "tot_reward, reward = 0,0\n",
    "\n",
    "# QLearning(env, 0.2, 0.9, 0.8, 0, 5000)\n",
    "\n",
    "it = 0       \n",
    "while done != True:   \n",
    "    # Render environment for last five episodes\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    # Determine next action - epsilon greedy strategy\n",
    "    if np.random.random() < 1 - epsilon:\n",
    "        action = np.argmax(Q[state_adj[0], state_adj[1]])  ## find the best action\n",
    "    else:\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "    # Get next state and reward\n",
    "    state2, reward, done, info = env.step(action) \n",
    "    \n",
    "#     print('action {}'.format(action))\n",
    "\n",
    "    # Discretize state2\n",
    "    state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "    state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "\n",
    "    #Allow for terminal states\n",
    "    if done and state2[0] >= 0.5:\n",
    "        Q[state_adj[0], state_adj[1], action] = reward\n",
    "\n",
    "    # Adjust Q value for current state\n",
    "    else:\n",
    "        delta = learning*(reward + \n",
    "                         discount*np.max(Q[state2_adj[0], \n",
    "                                           state2_adj[1]]) - \n",
    "                         Q[state_adj[0], state_adj[1],action]) ## update equation\n",
    "        Q[state_adj[0], state_adj[1],action] += delta ## q-table updated\n",
    "\n",
    "    # Update variables\n",
    "    tot_reward += reward\n",
    "    state_adj = state2_adj\n",
    "    it += 1\n",
    "    if (it % 1000 == 0):\n",
    "        print('it {}, reward {}'.format(it, reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
